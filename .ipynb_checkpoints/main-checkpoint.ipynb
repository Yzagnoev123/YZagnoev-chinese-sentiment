{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d983410-3d15-4bd2-b842-8913e40bf284",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import sentencepiece as spm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import random\n",
    "from datasets import concatenate_datasets\n",
    "from datasets import Dataset\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6556f2c-7ba4-42c6-b129-c24609f5d2fa",
   "metadata": {},
   "source": [
    "## Function For Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c6f55ac-c088-4a8e-adcf-202ed197d4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function is used for finding data needed on the internet for neutral_sentiment\n",
    "\n",
    "def neutral_sentiment_function(urls):\n",
    "    articles_content = []\n",
    "    for url in urls: #fill articles_content with content from url's\n",
    "        response = requests.get(url)\n",
    "        # Adjust encoding based on the site's specific encoding\n",
    "        response.encoding = 'UTF-8'\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        # Adjust the selection based on the content structure of the site\n",
    "        article_content = soup.find(\"div\", class_=\"mw-page-container-inner\") \n",
    "        if article_content:\n",
    "            articles_content.append(article_content.get_text())#creating a data frame of sentences\n",
    "   # nltk.download('punkt')\n",
    "    all_sentences = []\n",
    "    for content in articles_content:\n",
    "        sentences = sent_tokenize(content)\n",
    "        all_sentences.extend(sentences)\n",
    "    df = pd.DataFrame(all_sentences, columns=['text'])\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    print(df.head())\n",
    "    #df['data'] = df['text'].apply(lambda x: {'label': 2, 'text': x}) #every neutral will be referenced with a 2\n",
    "    #neutral_data = df['data'].tolist()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71359e79-6594-4fb6-b2fb-72a5d2d0dc0a",
   "metadata": {},
   "source": [
    "## Function For Combining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c86832b-cb5f-4d0d-96b3-c6c0a79283ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_data(df,sentiment_type,train_dataset,test_dataset):\n",
    "    sentiment_number = 0\n",
    "    if sentiment_type == 'positive':\n",
    "        sentiment_number = 1\n",
    "    elif sentiment_type == 'negative':\n",
    "        sentiment_number = 0\n",
    "    elif sentiment_type == 'neutral':\n",
    "        sentiment_number = 2\n",
    "\n",
    "    df['data'] = df['text'].apply(lambda x: {'label': sentiment_number, 'text': x}) #every neutral will be referenced with a 2\n",
    "    added_data = df['data'].tolist()\n",
    "    total_length = len(added_data)\n",
    "    split_index = int(total_length * 0.75)\n",
    "    added_training_data = added_data[:split_index]\n",
    "    data_dict = {key: [dic[key] for dic in added_training_data] for key in added_training_data[0]}\n",
    "    added_training_dataset = Dataset.from_dict(data_dict)\n",
    "    combined_training_dataset = concatenate_datasets([train_dataset,added_training_dataset])\n",
    "    train_dataset = combined_training_dataset.shuffle(seed = 42)\n",
    "\n",
    "    added_testing_data = added_data[split_index:]\n",
    "    data_dict = {key: [dic[key] for dic in added_testing_data] for key in added_testing_data[0]}\n",
    "    added_testing_dataset = Dataset.from_dict(data_dict)\n",
    "    combined_testing_dataset = concatenate_datasets([test_dataset,added_testing_dataset])\n",
    "    test_dataset = combined_testing_dataset.shuffle(seed=42)\n",
    "\n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e3b0c2-5a4c-4bb6-95c0-7c148e949c8f",
   "metadata": {},
   "source": [
    "## Loading the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4696b53a-f548-4dc9-93eb-f7c5ff96795d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'combined_train_dataset = concatenate_datasets([train_dataset,train_dataset_2]) \\ntrain_dataset = combined_train_dataset.shuffle(seed=42)'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "dataset = load_dataset(\"sepidmnorozy/Chinese_sentiment\")\n",
    "train_dataset = dataset[\"train\"]\n",
    "test_dataset = dataset[\"test\"]\n",
    "dataset_2 = load_dataset(\"t1annnnn/Chinese_sentimentAnalyze\") #this dataset gives a less accurate training\n",
    "#train_dataset_2 = dataset_2[\"train\"]\n",
    "test_dataset_2 = dataset_2[\"test\"] #when data from the second dataset is used to test the model trained on the first dataset, the results are not accurate\n",
    "'''combined_train_dataset = concatenate_datasets([train_dataset,train_dataset_2]) \n",
    "train_dataset = combined_train_dataset.shuffle(seed=42)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06603ecc-5112-4a4c-abd0-aaacd53d9b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length of 'text' sections: 129.95894607843138\n",
      "Average length of 'text' sections: 44.55753132352137\n"
     ]
    }
   ],
   "source": [
    "text_lengths = [len(entry['text']) for entry in test_dataset]\n",
    "text_lengths_2 = [len(entry['text']) for entry in test_dataset_2]\n",
    "# Assuming text_lengths is a list of integers representing the lengths of 'text' sections\n",
    "average_length = sum(text_lengths) / len(text_lengths)\n",
    "print(f\"Average length of 'text' sections: {average_length}\")\n",
    "average_length_2 = sum(text_lengths_2)/ len(text_lengths_2)\n",
    "print(f\"Average length of 'text' sections: {average_length_2}\") #the length of dataset 2 is shorter than dataset 1 - perhaps impacting accuracy?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a672264b-30d4-4d10-a6b3-f615e6de0b63",
   "metadata": {},
   "source": [
    "## Including a Neutral Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c695cb6-3175-45ad-a68c-cf7c03d1c446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text\n",
      "0  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n目录\\n移至侧栏\\n...\n",
      "1                                       《中原文化与民族复兴》.\n",
      "2                                           河南人民出版社.\n",
      "3                                         2010: 第7頁.\n",
      "4                                ISBN 9787215072756.\n"
     ]
    }
   ],
   "source": [
    "# Specify the URL\n",
    "#nltk.download('punkt')\n",
    "urls = [\"https://zh.wikipedia.org/wiki/%E4%B8%AD%E5%9C%8B\", \"https://zh.wikipedia.org/wiki/%E4%B8%AD%E5%8D%8E%E4%BA%BA%E6%B0%91%E5%85%B1%E5%92%8C%E5%9B%BD\"]\n",
    "df = neutral_sentiment_function(urls) #function used for scraping - see above\n",
    "sentiment_type = 'neutral'\n",
    "train_dataset,test_dataset = combine_data(df,sentiment_type,train_dataset,test_dataset) #ussed to combine neutral sentiment with the rest of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ddd250-3a04-49b9-a4a3-8c4f587b2551",
   "metadata": {},
   "source": [
    "## Tokenizing and Vectorizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cff2eb1e-1241-4a02-96d8-12ab2915936e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create and write to chinese_data.txt for SnetencePiece training\n",
    "with open(\"chinese_data.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    for example in train_dataset:\n",
    "        file.write(example[\"text\"] + \"\\n\")\n",
    "\n",
    "#Training the data set model:\n",
    "spm.SentencePieceTrainer.Train('--input=chinese_data.txt --model_prefix=chinese_model --vocab_size=8000 --character_coverage=0.9995 --model_type=bpe')\n",
    "\n",
    "# Load the trained SentencePiece model\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load(\"chinese_model.model\")\n",
    "tokenized_texts = [sp.EncodeAsPieces(text) for text in train_dataset['text']]\n",
    "tokenized_test_texts = [sp.EncodeAsPieces(text) for text in test_dataset['text']]\n",
    "joined_texts = [' '.join(tokens) for tokens in tokenized_texts]\n",
    "joined_test_texts = [' '.join(tokens) for tokens in tokenized_test_texts]\n",
    "'''tokenized_test_texts_2 = [sp.EncodeAsPieces(text) for text in test_dataset_2['text']]\n",
    "joined_test_texts_2 = [' '.join(tokens) for tokens in tokenized_test_texts_2]'''\n",
    "vectorizer = TfidfVectorizer(ngram_range = (1,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61805ffb-8849-4d6f-9546-eb5f7003ac24",
   "metadata": {},
   "source": [
    "## Creating Feature variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7627291b-2dcf-41ab-b525-199c31553c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the model and transforming the text data into TF-IDF vectors\n",
    "X_train = vectorizer.fit_transform(joined_texts)\n",
    "X_test = vectorizer.transform(joined_test_texts)\n",
    "#X_test_2 = vectorizer.transform(joined_test_texts_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20145a4-8cd9-49c0-9b5f-28efb0065621",
   "metadata": {},
   "source": [
    "## Creating Target Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ff088e2-e63e-4bcb-be44-be6b5d82af64",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = [data['label'] for data in train_dataset]\n",
    "y_test = [data['label'] for data in test_dataset]\n",
    "#y_test_2 = [data['label'] for data in test_dataset_2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08af23ab-ea3c-49ed-b0e6-5d44766f2dfb",
   "metadata": {},
   "source": [
    "## Creating and Testing LogisticRegression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17bd42a2-b528-4a48-b2c0-69802c830ad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9135633754697643\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.89      0.91      1921\n",
      "           1       0.88      0.96      0.92      2975\n",
      "           2       0.99      0.82      0.90       958\n",
      "\n",
      "    accuracy                           0.91      5854\n",
      "   macro avg       0.94      0.89      0.91      5854\n",
      "weighted avg       0.92      0.91      0.91      5854\n",
      "\n"
     ]
    }
   ],
   "source": [
    "default_model = LogisticRegression()\n",
    "\n",
    "# Train the model\n",
    "default_model.fit(X_train, y_train)\n",
    "# Evaluate the model\n",
    "default_predictions = default_model.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, default_predictions))\n",
    "print(classification_report(y_test, default_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "edda075e-b760-4917-a4d4-b8b9ce1c5e8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average correct length: 171.8335515242192\n",
      "Average incorrect length: 105.87326732673267\n",
      "▁^ ▁房 价 降 多 少 百 姓 才 能 买 得起 房 .\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "incorrect_texts = []\n",
    "correct_texts = []\n",
    "incorrect_predictions = []\n",
    "correct_labels = []\n",
    "\n",
    "#checking which lengths on average where incorrectly labelled\n",
    "\n",
    "for i in range(len(y_test)):\n",
    "    if default_predictions[i] != y_test[i]:\n",
    "        incorrect_texts.append(joined_test_texts[i])\n",
    "        incorrect_predictions.append(default_predictions[i])\n",
    "        correct_labels.append(y_test[i])\n",
    "    else:\n",
    "        correct_texts.append(joined_test_texts[i])\n",
    "\n",
    "average_incorrect_length = []\n",
    "sum_incorrect_length = 0\n",
    "for j in range(len(incorrect_texts)):\n",
    "    sum_incorrect_length = sum_incorrect_length + len(incorrect_texts[j])\n",
    "\n",
    "average_incorrect_length = sum_incorrect_length/j\n",
    "\n",
    "average_correct_length = []\n",
    "sum_correct_length = 0\n",
    "for h in range(len(correct_texts)):\n",
    "    sum_correct_length = sum_correct_length + len(correct_texts[h])\n",
    "\n",
    "average_correct_length = sum_correct_length/h\n",
    "\n",
    "print(f\"Average correct length: {average_correct_length}\")\n",
    "print(f\"Average incorrect length: {average_incorrect_length}\")\n",
    "print(incorrect_texts[5])\n",
    "print(len(incorrect_texts[5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae73076d-4486-44e5-8b49-032a7d9436c0",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (2939295078.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[12], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    '''model_1 = LogisticRegression(C=0.01, penalty = 'l1', solver = 'saga') #c=0.01\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "'''model_1 = LogisticRegression(C=0.01, penalty = 'l1', solver = 'saga') #c=0.01\n",
    "\n",
    "model_1.fit(X_train, y_train)\n",
    "# Evaluate the model\n",
    "model_1_predictions = model_1.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, model_1_predictions))\n",
    "print(classification_report(y_test, model_1_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554ade02-0c4b-4bae-a64d-91e6aa6fa003",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''model_1 = LogisticRegression(C=0.1, penalty = 'l1', solver = 'saga')#c=0.1\n",
    "\n",
    "model_1.fit(X_train, y_train)\n",
    "# Evaluate the model\n",
    "model_1_predictions = model_1.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, model_1_predictions))\n",
    "print(classification_report(y_test, model_1_predictions))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9cd143-bc22-4590-8622-cf0f000f8d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = LogisticRegression(C=1, penalty = 'l2', solver = 'saga') #c=1\n",
    "\n",
    "model_1.fit(X_train, y_train)\n",
    "# Evaluate the model\n",
    "model_1_predictions = model_1.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, model_1_predictions))\n",
    "print(classification_report(y_test, model_1_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bf149b-8af6-4912-a1a1-530e0849f9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = LogisticRegression(C=1, penalty = 'l1', solver = 'liblinear') #c=1\n",
    "\n",
    "model_1.fit(X_train, y_train)\n",
    "# Evaluate the model\n",
    "model_1_predictions = model_1.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, model_1_predictions))\n",
    "print(classification_report(y_test, model_1_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03bcba0-03d5-4a32-86e1-2bbe4c37e842",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression_model = LogisticRegression()\n",
    "\n",
    "'''param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100],\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'solver': ['liblinear', 'saga']  # solvers that support l1 penalty\n",
    "}\n",
    "\n",
    "print(\"gridsearch\")\n",
    "# Setup the grid search\n",
    "grid_search = GridSearchCV(logistic_regression_model, param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "print(\"fit\")\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and best score\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Score:\", grid_search.best_score_)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4fc6a7-8e2d-4c4d-bacc-9d141803db59",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = logistic_regression_model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9788be7c-d1e1-4d59-ae5f-ccd803741bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''incorrect_texts = []\n",
    "correct_texts = []\n",
    "incorrect_predictions = []\n",
    "correct_labels = []\n",
    "\n",
    "for i in range(len(y_test)):\n",
    "    if y_pred[i] != y_test[i]:\n",
    "        incorrect_texts.append(joined_test_texts[i])\n",
    "        incorrect_predictions.append(y_pred[i])\n",
    "        correct_labels.append(y_test[i])\n",
    "    else:\n",
    "        correct_texts.append(joined_test_texts[i])\n",
    "\n",
    "average_incorrect_length = []\n",
    "sum_incorrect_length = 0\n",
    "for j in range(len(incorrect_texts)):\n",
    "    sum_incorrect_length = sum_incorrect_length + len(incorrect_texts[j])\n",
    "\n",
    "average_incorrect_length = sum_incorrect_length/j\n",
    "\n",
    "average_correct_length = []\n",
    "sum_correct_length = 0\n",
    "for h in range(len(correct_texts)):\n",
    "    sum_correct_length = sum_correct_length + len(correct_texts[h])\n",
    "\n",
    "average_correct_length = sum_correct_length/h\n",
    "\n",
    "print(f\"Average correct length: {average_correct_length}\")\n",
    "print(f\"Average incorrect length: {average_incorrect_length}\")\n",
    "print(incorrect_texts[5])\n",
    "print(len(incorrect_texts[5]))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb13b87-0b7a-4b94-bbc5-2b51e05029b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''sgd_classifier = SGDClassifier(loss='hinge',  # Use 'hinge' for linear SVM, 'log' for logistic regression\n",
    "                               max_iter=1000,\n",
    "                               tol=1e-3,\n",
    "                               random_state=42)\n",
    "sgd_classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = sgd_classifier.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8feeaf-7c0b-4515-b216-fc054d76898b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''svm_classifier = SVC(kernel='linear') \n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = svm_classifier.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "print(classification_report(y_test, y_pred))'''\n",
    "\n",
    "'''linear_svc_model = LinearSVC(max_iter=1000) \n",
    "linear_svc_model.fit(X_train, y_train)\n",
    "y_pred = linear_svc_model.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6eddc4b-ea29-4fc9-8f1f-1c4f7336e183",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7acaa2-9fc6-4791-8935-4e1158a95946",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''naive_bayes_model = MultinomialNB()\n",
    "naive_bayes_model.fit(X_train, y_train)\n",
    "y_pred_NB = naive_bayes_model.predict(X_test)\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Calculate accuracy\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_NB))\n",
    "\n",
    "print(classification_report(y_test, y_pred_NB))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544bf9df-4117-44f8-92c4-f58107acedb4",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e8379b-5d4e-4dad-9433-d4da85e6caa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''random_forest_model = RandomForestClassifier(n_estimators=100, random_state=42)  \n",
    "random_forest_model.fit(X_train, y_train)\n",
    "y_pred_RF = random_forest_model.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "print(classification_report(y_test, y_pred))'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2545316-b9a3-408d-a5dd-64aa9eb8c8af",
   "metadata": {},
   "source": [
    "## SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cca78e0-4103-49bb-ad2b-c06f74d5aa97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sgd_classifier = SGDClassifier(alpha = 0.0001, loss = 'log_loss', learning_rate = 'optimal', random_state=42, penalty = 'l2')\n",
    "sgd_classifier = SGDClassifier()\n",
    "\n",
    "sgd_classifier.fit(X_train, y_train)\n",
    "y_pred = sgd_classifier.predict(X_test)\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d4d05a-69bf-4d13-bb35-6cae23693380",
   "metadata": {},
   "source": [
    "Hyperparameter searching"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
