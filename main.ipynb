{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "3d983410-3d15-4bd2-b842-8913e40bf284",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import sentencepiece as spm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import random\n",
    "from datasets import concatenate_datasets\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6556f2c-7ba4-42c6-b129-c24609f5d2fa",
   "metadata": {},
   "source": [
    "## Loading the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f4f91743-0bb0-4b76-869e-99e1ca45ddbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f01299de545c469580f42e1c2003593e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/3.53M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "367d2a19d59545b8b98800f76ed4e1df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/724k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "305a16d4df3c4b9c89f6ad8888c9e672",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2abb345b27e140ea8f9b8212ab7900ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c9d9305e18f453a921cb1fefa5d6004",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ac13d474529436cbfc434f49a5b914a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "#dataset = load_dataset(\"t1annnnn/Chinese_sentimentAnalyze\")\n",
    "dataset = load_dataset(\"sepidmnorozy/Chinese_sentiment\")\n",
    "train_dataset = dataset[\"train\"]\n",
    "test_dataset = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3108558d-b22f-4df5-ac92-0b535a6f771a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_2 = load_dataset(\"t1annnnn/Chinese_sentimentAnalyze\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b1114453-2b74-4c2d-badb-4ab81e4cf211",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_2 = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "06603ecc-5112-4a4c-abd0-aaacd53d9b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 1, 'text': '故事 情節 很好 很 唯美 我的 很多 同學 都說 很好 看 ， 但是 我認為 一本 好的 書 裡面 的 東西 也 很多 主題 有了 ， 但是 次 要 的 也 很 重要 的 ， 換 位 想 人的 一生 要有 很多 東西 不只是 愛情 。 我的 一位 同學 說 結尾 少點 東西 ， 我 個人 也是 這樣 認為 我 覺得 少了 女主角 的 親情 。 你說 也 感覺到 太 單調 ， 我 很 期待 那 本 《 代 孕 媽媽 》 我 看了 前面 我 覺得 比 你 前面 寫的 都好 裡面 有 東西 了 ， 內容 豐富 了 人物 真實 了 ， 這 說明 你的 筆鋒 成熟 了 。 加油 哦 ！'}\n"
     ]
    }
   ],
   "source": [
    "print(test_dataset_2[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "717a58b4-2698-4c38-b30f-9fef1bedb796",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Specify the URL\n",
    "urls = [\"https://zh.wikipedia.org/wiki/%E4%B8%AD%E5%9C%8B\", \"https://zh.wikipedia.org/wiki/%E4%B8%AD%E5%8D%8E%E4%BA%BA%E6%B0%91%E5%85%B1%E5%92%8C%E5%9B%BD\"]\n",
    "#url = \"https://zh.wikipedia.org/wiki/%E4%B8%AD%E5%9C%8B\"\n",
    "#url = \"https://zh.wikipedia.org/wiki/%E4%B8%AD%E5%8D%8E%E4%BA%BA%E6%B0%91%E5%85%B1%E5%92%8C%E5%9B%BD\"\n",
    "articles_content = []\n",
    "for url in urls: #fill articles_content with content from url's\n",
    "    response = requests.get(url)\n",
    "    # Adjust encoding based on the site's specific encoding\n",
    "    response.encoding = 'UTF-8'\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    # Adjust the selection based on the content structure of the site\n",
    "    article_content = soup.find(\"div\", class_=\"mw-page-container-inner\")  # Use the correct tag and class_ or id\n",
    "    if article_content:\n",
    "        articles_content.append(article_content.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "b632065b-29c0-44c6-b7c5-8e3bbe78b018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text\n",
      "0  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n目录\\n移至侧栏\\n...\n",
      "1                                       《中原文化与民族复兴》.\n",
      "2                                           河南人民出版社.\n",
      "3                                         2010: 第7頁.\n",
      "4                                ISBN 9787215072756.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Dell\n",
      "[nltk_data]     3520\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#creating a data frame of sentences\n",
    "nltk.download('punkt')\n",
    "all_sentences = []\n",
    "for content in articles_content:\n",
    "    sentences = sent_tokenize(content)\n",
    "    all_sentences.extend(sentences)\n",
    "df = pd.DataFrame(all_sentences, columns=['text'])\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "print(df.head())\n",
    "#df.to_csv(\"neutral_chinese_sentiment.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "82ac1a0e-a587-48d1-b72a-5da23c8ab115",
   "metadata": {},
   "outputs": [],
   "source": [
    "#turning dataframe into a dictionary\n",
    "#print(df.columns)\n",
    "#df.columns = ['text']  \n",
    "df['data'] = df['text'].apply(lambda x: {'label': 2, 'text': x}) #every neutral will be referenced with a 2\n",
    "neutral_data = df['data'].tolist()\n",
    "total_length = len(neutral_data)\n",
    "split_index = int(total_length * 0.75)\n",
    "neutral_training_data = neutral_data[:split_index]\n",
    "data_dict = {key: [dic[key] for dic in neutral_training_data] for key in neutral_training_data[0]}\n",
    "neutral_training_dataset = Dataset.from_dict(data_dict)\n",
    "combined_training_dataset = concatenate_datasets([train_dataset,neutral_training_dataset])\n",
    "train_dataset = combined_training_dataset.shuffle(seed = 42)\n",
    "\n",
    "neutral_testing_data = neutral_data[split_index:]\n",
    "data_dict = {key: [dic[key] for dic in neutral_testing_data] for key in neutral_testing_data[0]}\n",
    "neutral_testing_dataset = Dataset.from_dict(data_dict)\n",
    "combined_testing_dataset = concatenate_datasets([test_dataset,neutral_testing_dataset])\n",
    "test_dataset = combined_testing_dataset.shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ddd250-3a04-49b9-a4a3-8c4f587b2551",
   "metadata": {},
   "source": [
    "## Tokenizing and Vectorizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "3e6f9a70-c2ae-40cd-9299-e8445a5ac474",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create and write to chinese_data.txt for SnetencePiece training\n",
    "with open(\"chinese_data.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    for example in train_dataset:\n",
    "        file.write(example[\"text\"] + \"\\n\")\n",
    "\n",
    "#Training the data set model:\n",
    "spm.SentencePieceTrainer.Train('--input=chinese_data.txt --model_prefix=chinese_model --vocab_size=8000 --character_coverage=0.9995 --model_type=bpe')\n",
    "\n",
    "# Load the trained SentencePiece model\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load(\"chinese_model.model\")\n",
    "\n",
    "# Tokenize all texts in the training dataset\n",
    "tokenized_texts = [sp.EncodeAsPieces(text) for text in train_dataset['text']]\n",
    "#for testing\n",
    "tokenized_test_texts = [sp.EncodeAsPieces(text) for text in test_dataset['text']]\n",
    "\n",
    "# Converting token lists back to strings\n",
    "joined_texts = [' '.join(tokens) for tokens in tokenized_texts]\n",
    "# Joining the tokens into strings for each document in the test set\n",
    "joined_test_texts = [' '.join(tokens) for tokens in tokenized_test_texts]\n",
    "\n",
    "# Creating a TfidfVectorizer object\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9dd748db-c5d7-4b5b-bda5-871ad51ca832",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_test_texts_2 = [sp.EncodeAsPieces(text) for text in test_dataset_2['text']]\n",
    "joined_test_texts_2 = [' '.join(tokens) for tokens in tokenized_test_texts_2]\n",
    "vectorizer = TfidfVectorizer(ngram_range = (1,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61805ffb-8849-4d6f-9546-eb5f7003ac24",
   "metadata": {},
   "source": [
    "## Creating Feature variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "7627291b-2dcf-41ab-b525-199c31553c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the model and transforming the text data into TF-IDF vectors\n",
    "X_train = vectorizer.fit_transform(joined_texts)\n",
    "X_test = vectorizer.transform(joined_test_texts)\n",
    "X_test_2 = vectorizer.transform(joined_test_texts_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8d46a33-e973-4b02-b4fb-034b87d28c16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n# Calculate the sum of TF-IDF scores for each feature (word)\\nsum_tfidf = np.array(X.sum(axis=0)).flatten()\\n#print(sum_tfidf)\\n# Calculate the average by dividing by the number of documents\\naverage_tfidf = sum_tfidf / X.shape[0]\\n\\n# Get feature names to map indices to words\\nfeature_names = vectorizer.get_feature_names_out()\\n\\n# Map from feature name to average tf-idf score\\nword_to_avg_score = {feature_names[i]: average_tfidf[i] for i in range(len(feature_names))}\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "# Calculate the sum of TF-IDF scores for each feature (word)\n",
    "sum_tfidf = np.array(X.sum(axis=0)).flatten()\n",
    "#print(sum_tfidf)\n",
    "# Calculate the average by dividing by the number of documents\n",
    "average_tfidf = sum_tfidf / X.shape[0]\n",
    "\n",
    "# Get feature names to map indices to words\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Map from feature name to average tf-idf score\n",
    "word_to_avg_score = {feature_names[i]: average_tfidf[i] for i in range(len(feature_names))}\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20145a4-8cd9-49c0-9b5f-28efb0065621",
   "metadata": {},
   "source": [
    "## Creating Target Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "5ff088e2-e63e-4bcb-be44-be6b5d82af64",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = [data['label'] for data in train_dataset]\n",
    "y_test = [data['label'] for data in test_dataset]\n",
    "y_test_2 = [data['label'] for data in test_dataset_2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08af23ab-ea3c-49ed-b0e6-5d44766f2dfb",
   "metadata": {},
   "source": [
    "## Creating and Testing LogisticRegression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "17bd42a2-b528-4a48-b2c0-69802c830ad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9549041713641488\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.86      0.88      1921\n",
      "           1       0.92      0.94      0.93      2975\n",
      "           2       0.99      1.00      0.99      5748\n",
      "\n",
      "    accuracy                           0.95     10644\n",
      "   macro avg       0.94      0.93      0.93     10644\n",
      "weighted avg       0.95      0.95      0.95     10644\n",
      "\n"
     ]
    }
   ],
   "source": [
    "default_model = LogisticRegression()\n",
    "\n",
    "# Train the model\n",
    "default_model.fit(X_train, y_train)\n",
    "# Evaluate the model\n",
    "default_predictions = default_model.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, default_predictions))\n",
    "print(classification_report(y_test, default_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae73076d-4486-44e5-8b49-032a7d9436c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model_1 = LogisticRegression(C=0.01, penalty = \\'l1\\', solver = \\'saga\\') #c=0.01\\n\\nmodel_1.fit(X_train, y_train)\\n# Evaluate the model\\nmodel_1_predictions = model_1.predict(X_test)\\nprint(\"Accuracy:\", accuracy_score(y_test, model_1_predictions))\\nprint(classification_report(y_test, model_1_predictions))'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''model_1 = LogisticRegression(C=0.01, penalty = 'l1', solver = 'saga') #c=0.01\n",
    "\n",
    "model_1.fit(X_train, y_train)\n",
    "# Evaluate the model\n",
    "model_1_predictions = model_1.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, model_1_predictions))\n",
    "print(classification_report(y_test, model_1_predictions))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "554ade02-0c4b-4bae-a64d-91e6aa6fa003",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model_1 = LogisticRegression(C=0.1, penalty = \\'l1\\', solver = \\'saga\\')#c=0.1\\n\\nmodel_1.fit(X_train, y_train)\\n# Evaluate the model\\nmodel_1_predictions = model_1.predict(X_test)\\nprint(\"Accuracy:\", accuracy_score(y_test, model_1_predictions))\\nprint(classification_report(y_test, model_1_predictions))'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''model_1 = LogisticRegression(C=0.1, penalty = 'l1', solver = 'saga')#c=0.1\n",
    "\n",
    "model_1.fit(X_train, y_train)\n",
    "# Evaluate the model\n",
    "model_1_predictions = model_1.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, model_1_predictions))\n",
    "print(classification_report(y_test, model_1_predictions))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ee9cd143-bc22-4590-8622-cf0f000f8d7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7656617606828254\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.78      0.77      9089\n",
      "           1       0.78      0.75      0.76      9188\n",
      "\n",
      "    accuracy                           0.77     18277\n",
      "   macro avg       0.77      0.77      0.77     18277\n",
      "weighted avg       0.77      0.77      0.77     18277\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_1 = LogisticRegression(C=1, penalty = 'l2', solver = 'saga') #c=1\n",
    "\n",
    "model_1.fit(X_train, y_train)\n",
    "# Evaluate the model\n",
    "model_1_predictions = model_1.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, model_1_predictions))\n",
    "print(classification_report(y_test, model_1_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37bf149b-8af6-4912-a1a1-530e0849f9b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7644033484707556\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.78      0.77      9089\n",
      "           1       0.78      0.75      0.76      9188\n",
      "\n",
      "    accuracy                           0.76     18277\n",
      "   macro avg       0.76      0.76      0.76     18277\n",
      "weighted avg       0.76      0.76      0.76     18277\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_1 = LogisticRegression(C=1, penalty = 'l1', solver = 'liblinear') #c=1\n",
    "\n",
    "model_1.fit(X_train, y_train)\n",
    "# Evaluate the model\n",
    "model_1_predictions = model_1.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, model_1_predictions))\n",
    "print(classification_report(y_test, model_1_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c03bcba0-03d5-4a32-86e1-2bbe4c37e842",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'param_grid = {\\n    \\'C\\': [0.01, 0.1, 1, 10, 100],\\n    \\'penalty\\': [\\'l1\\', \\'l2\\'],\\n    \\'solver\\': [\\'liblinear\\', \\'saga\\']  # solvers that support l1 penalty\\n}\\n\\nprint(\"gridsearch\")\\n# Setup the grid search\\ngrid_search = GridSearchCV(logistic_regression_model, param_grid, cv=5, scoring=\\'accuracy\\')\\n\\nprint(\"fit\")\\n# Fit the grid search to the data\\ngrid_search.fit(X_train, y_train)\\n\\n# Best parameters and best score\\nprint(\"Best Parameters:\", grid_search.best_params_)\\nprint(\"Best Score:\", grid_search.best_score_)'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_regression_model = LogisticRegression()\n",
    "\n",
    "'''param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100],\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'solver': ['liblinear', 'saga']  # solvers that support l1 penalty\n",
    "}\n",
    "\n",
    "print(\"gridsearch\")\n",
    "# Setup the grid search\n",
    "grid_search = GridSearchCV(logistic_regression_model, param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "print(\"fit\")\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and best score\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Score:\", grid_search.best_score_)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae4fc6a7-8e2d-4c4d-bacc-9d141803db59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7650599113640094\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.78      0.77      9089\n",
      "           1       0.78      0.75      0.76      9188\n",
      "\n",
      "    accuracy                           0.77     18277\n",
      "   macro avg       0.77      0.77      0.77     18277\n",
      "weighted avg       0.77      0.77      0.77     18277\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logistic_regression_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = logistic_regression_model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9788be7c-d1e1-4d59-ae5f-ccd803741bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average correct length: 76.89114575883279\n",
      "Average incorrect length: 73.64779874213836\n",
      "▁回复 靖 添 尧 俺 家 员工 没有 退 役 潜 水 员 ▁ 靖 添 尧 为 你的 员工 负责 必须 米 有 ▁ 嗨 辣 老 孙 今儿 打 死 也没有 ▁张 玮 倩 孙 总 有 外 卖 么 ▁ 嗨 辣 老 孙 ▁回复 张 新 民 各种 鱼 各种 丸 儿 各种 贝 壳 儿 ▁张 新 民 都是 什么 货 色 啊\n",
      "155\n"
     ]
    }
   ],
   "source": [
    "incorrect_texts = []\n",
    "correct_texts = []\n",
    "incorrect_predictions = []\n",
    "correct_labels = []\n",
    "\n",
    "for i in range(len(y_test)):\n",
    "    if y_pred[i] != y_test[i]:\n",
    "        incorrect_texts.append(joined_test_texts[i])\n",
    "        incorrect_predictions.append(y_pred[i])\n",
    "        correct_labels.append(y_test[i])\n",
    "    else:\n",
    "        correct_texts.append(joined_test_texts[i])\n",
    "\n",
    "average_incorrect_length = []\n",
    "sum_incorrect_length = 0\n",
    "for j in range(len(incorrect_texts)):\n",
    "    sum_incorrect_length = sum_incorrect_length + len(incorrect_texts[j])\n",
    "\n",
    "average_incorrect_length = sum_incorrect_length/j\n",
    "\n",
    "average_correct_length = []\n",
    "sum_correct_length = 0\n",
    "for h in range(len(correct_texts)):\n",
    "    sum_correct_length = sum_correct_length + len(correct_texts[h])\n",
    "\n",
    "average_correct_length = sum_correct_length/h\n",
    "\n",
    "print(f\"Average correct length: {average_correct_length}\")\n",
    "print(f\"Average incorrect length: {average_incorrect_length}\")\n",
    "print(incorrect_texts[5])\n",
    "print(len(incorrect_texts[5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dbb13b87-0b7a-4b94-bbc5-2b51e05029b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sgd_classifier = SGDClassifier(loss=\\'hinge\\',  # Use \\'hinge\\' for linear SVM, \\'log\\' for logistic regression\\n                               max_iter=1000,\\n                               tol=1e-3,\\n                               random_state=42)\\nsgd_classifier.fit(X_train, y_train)\\n\\ny_pred = sgd_classifier.predict(X_test)\\n\\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred))\\nprint(classification_report(y_test, y_pred))'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''sgd_classifier = SGDClassifier(loss='hinge',  # Use 'hinge' for linear SVM, 'log' for logistic regression\n",
    "                               max_iter=1000,\n",
    "                               tol=1e-3,\n",
    "                               random_state=42)\n",
    "sgd_classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = sgd_classifier.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7f8feeaf-7c0b-4515-b216-fc054d76898b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'linear_svc_model = LinearSVC(max_iter=1000) \\nlinear_svc_model.fit(X_train, y_train)\\ny_pred = linear_svc_model.predict(X_test)\\n\\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred))\\nprint(classification_report(y_test, y_pred))'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''svm_classifier = SVC(kernel='linear') \n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = svm_classifier.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "print(classification_report(y_test, y_pred))'''\n",
    "\n",
    "'''linear_svc_model = LinearSVC(max_iter=1000) \n",
    "linear_svc_model.fit(X_train, y_train)\n",
    "y_pred = linear_svc_model.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6eddc4b-ea29-4fc9-8f1f-1c4f7336e183",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3c7acaa2-9fc6-4791-8935-4e1158a95946",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'naive_bayes_model = MultinomialNB()\\nnaive_bayes_model.fit(X_train, y_train)\\ny_pred_NB = naive_bayes_model.predict(X_test)\\nfrom sklearn.metrics import accuracy_score, classification_report\\n\\n# Calculate accuracy\\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred_NB))\\n\\nprint(classification_report(y_test, y_pred_NB))\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''naive_bayes_model = MultinomialNB()\n",
    "naive_bayes_model.fit(X_train, y_train)\n",
    "y_pred_NB = naive_bayes_model.predict(X_test)\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Calculate accuracy\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_NB))\n",
    "\n",
    "print(classification_report(y_test, y_pred_NB))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544bf9df-4117-44f8-92c4-f58107acedb4",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "36e8379b-5d4e-4dad-9433-d4da85e6caa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'random_forest_model = RandomForestClassifier(n_estimators=100, random_state=42)  \\nrandom_forest_model.fit(X_train, y_train)\\ny_pred_RF = random_forest_model.predict(X_test)\\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred))\\n\\nprint(classification_report(y_test, y_pred))'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''random_forest_model = RandomForestClassifier(n_estimators=100, random_state=42)  \n",
    "random_forest_model.fit(X_train, y_train)\n",
    "y_pred_RF = random_forest_model.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "print(classification_report(y_test, y_pred))'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2545316-b9a3-408d-a5dd-64aa9eb8c8af",
   "metadata": {},
   "source": [
    "## SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6cca78e0-4103-49bb-ad2b-c06f74d5aa97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9466911764705882\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.92      0.93      1921\n",
      "           1       0.95      0.96      0.96      2975\n",
      "\n",
      "    accuracy                           0.95      4896\n",
      "   macro avg       0.95      0.94      0.94      4896\n",
      "weighted avg       0.95      0.95      0.95      4896\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#sgd_classifier = SGDClassifier(alpha = 0.0001, loss = 'log_loss', learning_rate = 'optimal', random_state=42, penalty = 'l2')\n",
    "sgd_classifier = SGDClassifier()\n",
    "\n",
    "sgd_classifier.fit(X_train, y_train)\n",
    "y_pred = sgd_classifier.predict(X_test)\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d4d05a-69bf-4d13-bb35-6cae23693380",
   "metadata": {},
   "source": [
    "Hyperparameter searching"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
